/**
 * ü§ñ Servi√ßo de API LLM Externa (TinyLlama)
 * Integra√ß√£o com API LLM hospedada no servidor homelab.op:10650
 */

import { logger } from './logger.js';

class ExternalLLMService {
  constructor() {
    this.apiUrl = process.env.LLM_API_URL || 'http://homelab.op:10650/v1/chat/completions';
    this.defaultMaxTokens = parseInt(process.env.LLM_MAX_TOKENS) || 150;
    this.timeout = parseInt(process.env.LLM_TIMEOUT) || 30000; // 30 segundos

    logger.info(`ü§ñ Servi√ßo LLM configurado: ${this.apiUrl}`);
  }

  /**
   * üí¨ Gerar resposta de conversa√ß√£o
   */
  async generateConversation(options = {}) {
    try {
      const {
        prompt = '',
        context = '',
        maxTokens = this.defaultMaxTokens,
        temperature = 0.7,
        userId = null
      } = options;

      // Construir mensagens no formato Chat API
      const messages = [];
      
      // Adicionar contexto como system message se fornecido
      if (context && context.trim()) {
        messages.push({
          role: 'system',
          content: context.trim()
        });
      }

      // Adicionar prompt do usu√°rio
      messages.push({
        role: 'user',
        content: prompt.trim()
      });

      const requestData = {
        model: 'tinyllama_tinyllama-1.1b-chat-v1.0',
        messages,
        max_tokens: maxTokens
      };

      logger.info(`ü§ñ Gerando resposta LLM para: "${prompt.substring(0, 50)}..."`);
      
      const response = await fetch(this.apiUrl, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify(requestData),
        signal: AbortSignal.timeout(this.timeout)
      });
      
      if (!response.ok) {
        throw new Error(`API LLM retornou ${response.status}: ${response.statusText}`);
      }
      
      const data = await response.json();
      
      if (!data || !data.choices || !data.choices[0]) {
        throw new Error('Formato de resposta inv√°lido da API LLM');
      }

      let generatedText = data.choices[0].message.content;
      
      // Limpar tokens de template do TinyLlama
      generatedText = this.cleanLLMResponse(generatedText);

      const result = {
        content: generatedText,
        response: generatedText, // Manter compatibilidade
        tokenCount: data.usage?.completion_tokens || generatedText.split(' ').length,
        tokens: data.usage?.completion_tokens || generatedText.split(' ').length,
        source: 'external_llm',
        model: 'tinyllama',
        userId: userId || 'unknown',
        timestamp: new Date().toISOString()
      };

      logger.success(`‚úÖ Resposta LLM gerada: ${result.tokens} tokens`);
      return result;

    } catch (error) {
      logger.error('‚ùå Erro ao gerar resposta LLM:', error.message);
      
      // Fallback para resposta de erro
      return {
        content: 'Desculpe, n√£o consegui processar sua solicita√ß√£o no momento.',
        response: 'Desculpe, n√£o consegui processar sua solicita√ß√£o no momento.',
        error: error.message,
        tokenCount: 0,
        tokens: 0,
        source: 'external_llm',
        model: 'tinyllama',
        userId: userId || 'unknown',
        timestamp: new Date().toISOString()
      };
    }
  }

  /**
   * üìù Gerar texto livre
   */
  async generateText(options = {}) {
    const {
      prompt,
      maxTokens = this.defaultMaxTokens,
      context = ''
    } = options;

    return await this.generateConversation({
      prompt,
      context,
      maxTokens,
      ...options
    });
  }

  /**
   * üé≠ An√°lise de sentimento
   */
  async analyzeSentiment(text) {
    try {
      const prompt = `Analise o sentimento do seguinte texto e responda apenas com: "positivo", "negativo" ou "neutro".

Texto: "${text}"

Sentimento:`;

      const result = await this.generateConversation({
        prompt,
        maxTokens: 10,
        context: 'Voc√™ √© um analisador de sentimentos. Responda apenas com uma palavra: positivo, negativo ou neutro.'
      });

      const sentiment = result.response.toLowerCase().trim();
      
      // Mapear resposta para formato esperado
      let mappedSentiment = 'neutral';
      if (sentiment.includes('positivo')) mappedSentiment = 'positive';
      else if (sentiment.includes('negativo')) mappedSentiment = 'negative';

      return {
        sentiment: mappedSentiment,
        confidence: 0.8, // Confian√ßa simulada
        source: 'external_llm'
      };

    } catch (error) {
      logger.error('‚ùå Erro na an√°lise de sentimento:', error.message);
      return {
        sentiment: 'neutral',
        confidence: 0.5,
        source: 'external_llm',
        error: error.message
      };
    }
  }

  /**
   * üòä An√°lise de emo√ß√£o
   */
  async analyzeEmotion(text) {
    try {
      const prompt = `Identifique a emo√ß√£o principal no seguinte texto. Responda apenas com uma das op√ß√µes: alegria, tristeza, raiva, medo, surpresa, nojo, neutro.

Texto: "${text}"

Emo√ß√£o:`;

      const result = await this.generateConversation({
        prompt,
        maxTokens: 10,
        context: 'Voc√™ √© um analisador de emo√ß√µes. Responda apenas com uma palavra da lista fornecida.'
      });

      const emotion = result.response.toLowerCase().trim();
      
      // Mapear para emo√ß√µes padr√£o
      const emotions = ['alegria', 'tristeza', 'raiva', 'medo', 'surpresa', 'nojo', 'neutro'];
      const detectedEmotion = emotions.find(e => emotion.includes(e)) || 'neutro';

      return {
        emotion: detectedEmotion,
        confidence: 0.8,
        source: 'external_llm'
      };

    } catch (error) {
      logger.error('‚ùå Erro na an√°lise de emo√ß√£o:', error.message);
      return {
        emotion: 'neutro',
        confidence: 0.5,
        source: 'external_llm',
        error: error.message
      };
    }
  }

  /**
   * üßπ Limpar resposta do LLM (remover tokens de template)
   */
  cleanLLMResponse(text) {
    if (!text) return '';
    
    return text
      .replace(/<\|user\|>/g, '')
      .replace(/<\|assistant\|>/g, '')
      .replace(/<\|system\|>/g, '')
      .trim();
  }

  /**
   * üîç Testar conectividade da API
   */
  async testConnection() {
    try {
      const testResult = await this.generateConversation({
        prompt: 'Ol√°',
        maxTokens: 10,
        userId: 'test-connection'
      });
      
      return {
        connected: true,
        response: testResult.response,
        latency: Date.now() - Date.now() // Placeholder
      };
      
    } catch (error) {
      logger.error('‚ùå Teste de conex√£o LLM falhou:', error.message);
      return {
        connected: false,
        error: error.message
      };
    }
  }

  /**
   * üìä Obter status do servi√ßo
   */
  getStatus() {
    return {
      service: 'external_llm',
      apiUrl: this.apiUrl,
      maxTokens: this.defaultMaxTokens,
      timeout: this.timeout,
      model: 'tinyllama'
    };
  }
}

// Inst√¢ncia singleton
const externalLLMService = new ExternalLLMService();

export { externalLLMService, ExternalLLMService };
export default externalLLMService;